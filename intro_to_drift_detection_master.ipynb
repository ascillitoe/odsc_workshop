{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c6f990",
   "metadata": {},
   "source": [
    "TODO's:\n",
    "- Upload utility script somewhere, then have notebook fetch it (if it doesn't exist locally).\n",
    "- Check Classifier performane on GPU (after tokenize moved to .foward). Does x need to be sent to device?\n",
    "- Classifier save/load (inc. classifier trained on all data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a288462",
   "metadata": {},
   "source": [
    "# ODSC 2022: An Introduction to Drift Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c9ceb",
   "metadata": {},
   "source": [
    "## 0. Getting ready\n",
    "\n",
    "https://github.com/ascillitoe/odsc_workshop\n",
    "\n",
    "```\n",
    "git clone https://github.com/ascillitoe/odsc_workshop.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "09fa8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import zipfile\n",
    "    import shutil\n",
    "    import os\n",
    "\n",
    "    !wget -nc https://github.com/ascillitoe/odsc_workshop/archive/refs/heads/main.zip\n",
    "\n",
    "    pz = open('main.zip', 'rb')\n",
    "    packz = zipfile.ZipFile(pz)\n",
    "    packz.extractall()\n",
    "    pz.close()\n",
    "\n",
    "    srcdir = 'odsc_workshop-main/'\n",
    "    for filepath in os.listdir(srcdir):\n",
    "        if not os.path.exists(filepath):\n",
    "            shutil.copyfile(os.path.join(srcdir, filepath), filepath)\n",
    "    shutil.rmtree(srcdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4617fcb",
   "metadata": {},
   "source": [
    "### Software\n",
    "\n",
    "In this workshop we'll make use of state-of-the-art drift detectors from the open-source [Alibi Detect]() library. This can be installed (along with the PyTorch backend) via `pip`:\n",
    "\n",
    "```\n",
    "pip install alibi-detect[torch]\n",
    "```\n",
    "\n",
    "We'll also use a number of other packages which can also be installed with `pip`:\n",
    "\n",
    "```\n",
    "pip install umap-learn sentence-transformers statsmodels seaborn datasets scipy tqdm\n",
    "```\n",
    "\n",
    "You can also install the required dependencies by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65feb2",
   "metadata": {},
   "source": [
    "### Download data and sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch preqrequisites\n",
    "from workshop_utilities import fetch_prerequisites\n",
    "dataset, sentence_transformer = fetch_prerequisites()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2fd99",
   "metadata": {},
   "source": [
    "## 1. Classifying newsgroups\n",
    "\n",
    "### The data\n",
    "\n",
    "The 20 newsgroup dataset, which contains about 18,000 newsgroup posts across 20 topics, including politics, science sports and religion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(dataset.data)} documents')\n",
    "print(f'{len(dataset.target_names)} categories:')\n",
    "classes = dataset.target_names\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e47f1c",
   "metadata": {},
   "source": [
    "Let's take a look at an instance from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "for _, document in enumerate(dataset.data[:n]):\n",
    "    category = dataset.target_names[dataset.target[_]]\n",
    "    print(f'{_}. Category: {category}')\n",
    "    print('---------------------------')\n",
    "    print(document[:1000])\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27f700",
   "metadata": {},
   "source": [
    "### Visualising the embeddings\n",
    "\n",
    "We embed the news posts using [SentenceTransformers](https://www.sbert.net/index.html) pre-trained embeddings and optionally add a dimensionality reduction step with [UMAP](https://umap-learn.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from workshop_utilities import set_seed\n",
    "set_seed(2022)  # This will ensure reproducibility (at least on CPU!)\n",
    "n_all = len(dataset.data)\n",
    "\n",
    "n_train = 5000  # can be reduced if too slow on cpu\n",
    "\n",
    "idx_train = np.random.choice(n_all, size=n_train, replace=False)\n",
    "x_train, y_train = [dataset.data[_] for _ in idx_train], dataset.target[idx_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ea9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_utilities import EmbeddingModel\n",
    "emb_model = EmbeddingModel(model=sentence_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train = emb_model(x_train)\n",
    "emb_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93cb96",
   "metadata": {},
   "source": [
    "By applying UMAP on the *SentenceTransformer* embeddings, we can visually inspect the various news topic clusters. UMAP is able to take advantage of our data labels (i.e. `y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_utilities import UMAPModel, plot_clusters\n",
    "umap_model = UMAPModel()\n",
    "umap_model.fit(emb_train, y=y_train)\n",
    "dr_train = umap_model.predict(emb_train)\n",
    "dr_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(dr_train, y_train, classes, title='Training data: clustered news topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ebc30",
   "metadata": {},
   "source": [
    "### Training a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d394b2dc",
   "metadata": {},
   "source": [
    "First we train a classifier on a small subset of the data. The aim of the classifier is to predict the news topic of each instance.\n",
    "\n",
    "Let's train our classifier. The classifier consists of a simple MLP head on top of a pre-trained SentenceTransformer model as the backbone. The SentenceTransformer remains frozen during training and only the MLP head is finetuned.\n",
    "\n",
    "TODO - option to load already trained classifier (need to incorperate into prereq's too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eec98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from workshop_utilities import set_seed, split_data\n",
    "set_seed(2022)  # This will ensure reproducibility (at least on CPU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f64f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(classes)\n",
    "n_train_c = [0] * n_classes\n",
    "n_test_c = [0] * n_classes\n",
    "\n",
    "n_train_c[5], n_train_c[11] = 200, 200  # \n",
    "n_test_c[5], n_test_c[11] = 100, 100  # \n",
    "\n",
    "(x_train, y_train), (x_test, y_test), _ = split_data(dataset.data, dataset.target, n_train_c, n_test_c, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff64cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train = emb_model(x_train)\n",
    "plot_clusters(emb_train, y_train, classes, dr_model=umap_model, title='Training data: clustered news topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a679e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from workshop_utilities import Classifier, train_model, eval_model\n",
    "\n",
    "TRAIN_CLF = False  # Set to TRUE to train classifier, otherwise it will be loaded from disk\n",
    "filepath = 'classifier'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if TRAIN_CLF:\n",
    "    # init model\n",
    "    clf = Classifier().to(device)\n",
    "\n",
    "    # Train model\n",
    "    train_model(clf, x_train, y_train, epochs=5, shuffle=True)\n",
    "    clf.eval()\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(clf.state_dict(), filepath)\n",
    "else:\n",
    "    # Load model\n",
    "    clf = Classifier()\n",
    "    clf.load_state_dict(torch.load(filepath))\n",
    "    clf = clf.to(device)\n",
    "    clf.eval()\n",
    "\n",
    "_, _ = eval_model(clf, x_train, y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63002d4",
   "metadata": {},
   "source": [
    "### Testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddda901",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_test = emb_model(x_test)\n",
    "plot_clusters(emb_test, y_test, classes, dr_model=umap_model, title='Test data: clustered news topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef79da1",
   "metadata": {},
   "source": [
    "Test some classifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec4bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "print(x_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = clf([x_test[idx]]).argmax(1)\n",
    "classes[class_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "print(x_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be95ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = clf([x_test[idx]]).argmax(1)\n",
    "classes[class_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7b6d3",
   "metadata": {},
   "source": [
    "## 2. Detecting drift\n",
    "\n",
    "### Introducing drift\n",
    "\n",
    "Model uncertainty and supervised w/ FET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(classes)\n",
    "n_nodrift_c = n_test_c\n",
    "n_drift_c = [0] * n_classes\n",
    "\n",
    "n_drift_c[5], n_drift_c[11], n_drift_c[14] = 100, 100, 100  # \n",
    "\n",
    "(x_nodrift, y_nodrift), (x_drift, y_drift), _ = split_data(dataset.data, dataset.target, n_nodrift_c, n_drift_c, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d21d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_drift = emb_model(x_drift)\n",
    "plot_clusters(emb_drift, y_drift, classes, dr_model=umap_model, title='Drifted data: clustered news topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7673371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unique(y_drift).astype(int)\n",
    "np.array(classes)[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad48493",
   "metadata": {},
   "source": [
    "### Detecting model drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c392e",
   "metadata": {},
   "source": [
    "If we have labels can monitor model performance directly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = eval_model(clf, x_nodrift, y_nodrift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = eval_model(clf, x_drift, y_drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e48b9",
   "metadata": {},
   "source": [
    "Otherwise we can use model uncertainty as a proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import ClassifierUncertaintyDrift\n",
    "\n",
    "def preprocess_batch(x):\n",
    "    return clf.embedding_model(x)\n",
    "\n",
    "dd = ClassifierUncertaintyDrift(x_test, clf, \n",
    "                                preprocess_batch_fn=preprocess_batch, backend='pytorch', \n",
    "                                p_val=.05, preds_type='logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26423a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.predict(x_nodrift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57005484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.predict(x_drift)['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51888f2",
   "metadata": {},
   "source": [
    "## 3. Detecting drift on the inputs\n",
    "\n",
    "basic MMDDrift use\n",
    "\n",
    "Use UMAP to viz drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(x):\n",
    "    x = clf.embedding_model(x)\n",
    "    return x.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4aea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import MMDDrift\n",
    "dd = MMDDrift(x_test, backend='pytorch', p_val=.05, preprocess_fn=preprocess_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.predict(x_nodrift)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005cff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.predict(x_drift)['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28d50a",
   "metadata": {},
   "source": [
    "## 4. Accounting for context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7e40a",
   "metadata": {},
   "source": [
    "rgrgrhg\n",
    "\n",
    "what happens if training \n",
    "\n",
    "###  Changing the relative subpopulation prevalence\n",
    "\n",
    "relative frequency of one or more subpopulations (i.e. news topics) is changing in a way which can be attributed to external events. Importantly, the distribution underlying each subpopulation (e.g. the distribution of *hockey* news itself) remains unchanged, only its frequency changes.\n",
    "\n",
    "In our example we assume that the World Series and Stanley Cup coincide on the calendar leading to a spike in news articles on respectively baseball and hockey. Furthermore, there is not too much news on Mac or Windows since there are no new releases or products planned anytime soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(classes)\n",
    "n_nochange_c = 1000 // n_classes  # equally subsample each class from 1000 instances\n",
    "\n",
    "n_change_c = [50] * n_classes  # 100 of each class (but then mod. below)\n",
    "n_change_c[4], n_change_c[5] = 25, 25  # few stories on Mac/Windows\n",
    "n_change_c[9], n_change_c[10] = 75, 75  # more stories on baseball/hockey\n",
    "\n",
    "(x_nochange, y_nochange), (x_change, y_change), (x_held, y_held) = split_data(dataset.data, dataset.target, n_nochange_c, n_change_c, seed=0)\n",
    "\n",
    "# Split remaining data into train/ref\n",
    "idx_ref = np.random.choice(len(x_held), size=1000, replace=False)\n",
    "x_ref, y_ref = [x_held[_] for _ in idx_ref], y_held[idx_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_ref = emb_model(x_ref)\n",
    "plot_clusters(emb_ref, y_ref, classes, dr_model=umap_model, title='Reference data: clustered news topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6b2c9",
   "metadata": {},
   "source": [
    "### Vanilla MMD detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = MMDDrift(x_ref, p_val=0.05, preprocess_fn=preprocess_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5338a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.predict(x_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b3e80",
   "metadata": {},
   "source": [
    "The MMD detector consistently flags drift (low p-values). Note that this is the expected behaviour since the vanilla MMD detector cannot take any external context into account and correctly detects that the reference and test data do not follow the same underlying distribution.\n",
    "\n",
    "However... not necesarilly drift we want to detect. Classifier should work fine on this data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f21ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CLF = True  # Set to TRUE to train classifier, otherwise it will be loaded from disk\n",
    "filepath = 'classifier_full'\n",
    "if TRAIN_CLF:\n",
    "    # init model\n",
    "    clf_full = Classifier().to(device)\n",
    "\n",
    "    # Train model\n",
    "    train_model(clf_full, x_train, y_train, epochs=5, shuffle=True)\n",
    "    clf_full.eval()\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(clf_full.state_dict(), filepath)\n",
    "else:\n",
    "    # Load model\n",
    "    clf_full = Classifier()\n",
    "    clf_full.load_state_dict(torch.load(filepath)).to(device)\n",
    "    clf_full.eval()\n",
    "\n",
    "_, _ = eval_model(clf_full, x_nochange, y_nochange)\n",
    "_, _ = eval_model(clf_full, x_change, y_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c29960",
   "metadata": {},
   "source": [
    "### Context aware MMD detector\n",
    "\n",
    "We want to detect drift, given...\n",
    "\n",
    "To achieve this we **condition on the prediction probabilities of the classifier we trained earlier to distinguish each of the 20 different news topics**. We can do this because the prediction probabilities can account for the frequency of occurrence of each of the topics (be it imperfectly given our classifier makes the occasional mistake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(x):\n",
    "    \"\"\" Condition on classifier prediction probabilities. \"\"\"\n",
    "    logits = clf(x)\n",
    "    softmax_fn = torch.nn.Softmax(dim=-1)\n",
    "    return softmax_fn(logits).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import ContextMMDDrift\n",
    "dd_cad = ContextMMDDrift(x_ref, context(x_ref), p_val=.05, n_permutations=100, \n",
    "                         preprocess_fn=preprocess_fn, backend='pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cad.predict(x_change, context(x_change))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f02c43",
   "metadata": {},
   "source": [
    "Hopefully drift isn't detected! (hint: it might be sometimes, as we expect a uniform distribution of p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1d5bb",
   "metadata": {},
   "source": [
    "## Homework!\n",
    "\n",
    "### Examining detector calibration\n",
    "\n",
    "Before we set off our experiments, we compute all necessary embeddings and contexts so we don't have to run the embedding model on every loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exp = 5000  # This dataset is going to be split further in the loop, so it needs to be relatively large\n",
    "idx = np.random.choice(n_all, size=n_exp, replace=False)\n",
    "x_exp = [dataset.data[_] for _ in idx]\n",
    "\n",
    "emb_exp, c_exp = emb_model(x_exp).cpu().numpy(), context(x_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_runs = 50  # number of drift detection runs, each with a different reference and test sample\n",
    "n_ref, n_test = 1000, 500\n",
    "\n",
    "p_vals_mmd, p_vals_cad = [], []\n",
    "for _ in tqdm(range(n_runs)):\n",
    "    \n",
    "    # sample data\n",
    "    idx = np.random.choice(n_exp, size=n_exp, replace=False)\n",
    "    idx_ref, idx_test = idx[:n_ref], idx[n_ref:n_ref+n_test]\n",
    "    emb_ref, c_ref = emb_exp[idx_ref], c_exp[idx_ref]\n",
    "    emb_test, c_test = emb_exp[idx_test], c_exp[idx_test]\n",
    "    \n",
    "    # mmd drift detector\n",
    "    dd_mmd = MMDDrift(emb_ref, p_val=.05, n_permutations=100, backend='pytorch')\n",
    "    preds_mmd = dd_mmd.predict(emb_test)\n",
    "    p_vals_mmd.append(preds_mmd['data']['p_val'])\n",
    "    \n",
    "    # context-aware mmd drift detector \n",
    "    dd_cad = ContextMMDDrift(emb_ref, c_ref, p_val=.05, n_permutations=100, backend='pytorch')\n",
    "    preds_cad = dd_cad.predict(emb_test, c_test)\n",
    "    p_vals_cad.append(preds_cad['data']['p_val'])\n",
    "    \n",
    "p_vals_mmd = np.array(p_vals_mmd)\n",
    "p_vals_cad = np.array(p_vals_cad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715dbda",
   "metadata": {},
   "source": [
    "The below figure of the [Q-Q (Quantile-Quantile) plots](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) of a random sample from the uniform distribution *U[0,1]* against the obtained p-values from the vanilla and context-aware MMD detectors illustrate how well both detectors are calibrated. A perfectly calibrated detector should have a Q-Q plot which closely follows the diagonal. Only the middle plot in the grid shows the detector's p-values. The other plots correspond to *n_runs* p-values actually sampled from *U[0,1]* to contextualise how well the central plot follows the diagonal given the limited number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b40539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_utilities import plot_qq\n",
    "plot_qq(p_vals_mmd, 'Q-Q plot MMD detector')\n",
    "plot_qq(p_vals_cad, 'Q-Q plot Context-Aware MMD detector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734e435",
   "metadata": {},
   "source": [
    "As expected we can see that the context-aware MMD detectors is well-calibrated, but the normal MMD isn't!\n",
    "\n",
    "The same can be seen using histogram's of p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_utilities import plot_hist\n",
    "p_vals = [p_vals_mmd, p_vals_cad]\n",
    "title = 'p-value distribution for a change in subpopulation prevalence'\n",
    "plot_hist(p_vals, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e785b24",
   "metadata": {},
   "source": [
    "Test power can be quantified in a similar way, but here we want to examine... see..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64453f6",
   "metadata": {},
   "source": [
    "### Changing the subpopulation distribution\n",
    "\n",
    "See https://docs.seldon.io/projects/alibi-detect/en/latest/examples/cd_context_20newsgroup.html#Changing-the-subpopulation-distribution for an example where the distribution of the subpopulation has actually changed. In this case, we expect the context aware detector is expected to detect drift.\n",
    "\n",
    "### Changing the context\n",
    "\n",
    "See \n",
    "\n",
    "### Interpretability of detections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
